{
  "model_path": "models/llama/llama-7b.Q4_K_M.gguf",
  "chat_format": "llama-2",
  "context_window": 4096,
  "max_tokens": 800,
  "temperature": 0.4,
  "use_metal": true,
  "verbose": false,
  "n_gpu_layers": -1 
}
